{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c5bc1a",
   "metadata": {},
   "source": [
    "# ü•ã FalkorDB Agent with Pydantic AI - UFC Knowledge Graph Demo\n",
    "\n",
    "This notebook demonstrates how to create an intelligent **UFC-focused AI agent** using **FalkorDB** and **Pydantic AI** that can query a comprehensive UFC knowledge graph powered by the **GraphRAG SDK**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to set up a Pydantic AI agent with custom tools\n",
    "- How to integrate FalkorDB with the GraphRAG SDK\n",
    "- How to create a knowledge graph search tool for your agent\n",
    "- How to build an interactive chat interface with streaming responses\n",
    "- How to handle dependencies and error management in agent workflows\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "- A running FalkorDB instance with UFC data loaded (see [demo-ufc.ipynb](./demo-ufc.ipynb))\n",
    "- OpenAI API key for the language model\n",
    "- Python environment with required packages (installed below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eeeaf6",
   "metadata": {},
   "source": [
    "## üîß Installation Requirements\n",
    "\n",
    "This notebook requires three main packages. Install them with the cell below:\n",
    "\n",
    "### Required Packages:\n",
    "- **GraphRAG SDK**: Provides FalkorDB integration, Rich console formatting, OpenAI client, and graph operations\n",
    "- **Pydantic AI**: Modern AI agent framework with type safety and structured data handling  \n",
    "- **Gradio**: Web interface framework for creating interactive chat applications\n",
    "\n",
    "\n",
    "**Note**: The GraphRAG SDK includes most dependencies we need, so the installation is streamlined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ff4ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepeval 2.6.7 requires anthropic<0.50.0,>=0.49.0, but you have anthropic 0.55.0 which is incompatible.\n",
      "google-genai 1.22.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
      "mistralai 1.8.2 requires httpx>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepeval 2.6.7 requires anthropic<0.50.0,>=0.49.0, but you have anthropic 0.55.0 which is incompatible.\n",
      "ollama 0.2.1 requires httpx<0.28.0,>=0.27.0, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepeval 2.6.7 requires anthropic<0.50.0,>=0.49.0, but you have anthropic 0.55.0 which is incompatible.\n",
      "ollama 0.2.1 requires httpx<0.28.0,>=0.27.0, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install graphrag_sdk --quiet\n",
    "!pip install pydantic-ai --quiet\n",
    "!pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56eedd2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our FalkorDB and Pydantic AI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77206f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List\n",
    "from falkordb import FalkorDB\n",
    "from dataclasses import dataclass\n",
    "from __future__ import annotations\n",
    "from pydantic import BaseModel, Field\n",
    "from graphrag_sdk import KnowledgeGraph\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from graphrag_sdk.ontology import Ontology\n",
    "from graphrag_sdk.models.litellm import LiteModel\n",
    "from graphrag_sdk.chat_session import ChatSession\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from graphrag_sdk.model_config import KnowledgeGraphModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fbb3e",
   "metadata": {},
   "source": [
    "## 2. Load Environment Variables and Configuration\n",
    "\n",
    "Load environment variables for API keys and database connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "MODEL_CHOICE = 'gpt-4o-mini'\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"  # Replace with your actual API key or use os.getenv\n",
    "\n",
    "# FalkorDB connection parameters\n",
    "FALKORDB_HOST = \"localhost\" # Replace with your FalkorDB host\n",
    "FALKORDB_PORT = 6379  # Default port for FalkorDB\n",
    "FALKORDB_USERNAME = \"your-falkordb-username\"  # can be None if not required\n",
    "FALKORDB_PASSWORD = \"your-falkordb-password\"  # can be None if not required\n",
    "\n",
    "GRAPH_NAME = \"ufc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73cf2d",
   "metadata": {},
   "source": [
    "## 3. Set Up the LLM Model\n",
    "\n",
    "Configure the OpenAI model that will power our Pydantic AI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4658d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Configure and return the LLM model to use.\"\"\"\n",
    "    return OpenAIModel(MODEL_CHOICE, provider=OpenAIProvider(api_key=OPENAI_API_KEY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c9697",
   "metadata": {},
   "source": [
    "## 4. Connect to FalkorDB and Load Ontology\n",
    "\n",
    "Establish connection to FalkorDB and load the existing UFC knowledge graph ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbb3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to FalkorDB...\n",
      "‚úÖ Loaded ontology from existing knowledge graph.\n",
      "   Entities: 5\n",
      "   Relations: 7\n"
     ]
    }
   ],
   "source": [
    "# Connect to FalkorDB\n",
    "print(\"üîå Connecting to FalkorDB...\")\n",
    "db = FalkorDB(host=FALKORDB_HOST, port=FALKORDB_PORT, username=FALKORDB_USERNAME, password=FALKORDB_PASSWORD)\n",
    "graph = db.select_graph(GRAPH_NAME)\n",
    "\n",
    "# Load ontology from existing graph\n",
    "try:\n",
    "    ontology = Ontology.from_kg_graph(graph)\n",
    "    print(\"‚úÖ Loaded ontology from existing knowledge graph.\")\n",
    "    print(f\"   Entities: {len(ontology.entities) if ontology.entities else 0}\")\n",
    "    print(f\"   Relations: {len(ontology.relations) if ontology.relations else 0}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load ontology from existing graph: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33122bd",
   "metadata": {},
   "source": [
    "## 5. Initialize KnowledgeGraph and ChatSession\n",
    "\n",
    "Set up the GraphRAG SDK components for knowledge graph interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff61d0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Initializing KnowledgeGraph client...\n",
      "‚úÖ ChatSession created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LiteModel for GraphRAG SDK (by default - gpt-4.1)\n",
    "model_falkor = LiteModel()\n",
    "\n",
    "# Initialize model configuration\n",
    "model_config = KnowledgeGraphModelConfig.with_model(model_falkor)\n",
    "\n",
    "# Initialize KnowledgeGraph\n",
    "print(\"üîó Initializing KnowledgeGraph client...\")\n",
    "kg_client = KnowledgeGraph(\n",
    "    name=GRAPH_NAME,\n",
    "    model_config=model_config,\n",
    "    ontology=ontology,\n",
    "    host=FALKORDB_HOST,\n",
    "    port=FALKORDB_PORT,\n",
    "    username=FALKORDB_USERNAME,\n",
    "    password=FALKORDB_PASSWORD\n",
    ")\n",
    "\n",
    "# Create a chat session for agent queries (replaces cypher_session)\n",
    "chat_session = kg_client.chat_session()\n",
    "print(\"‚úÖ ChatSession created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a0874",
   "metadata": {},
   "source": [
    "## 6. Define Agent Dependencies\n",
    "\n",
    "Create a dependencies class to pass the ChatSession to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c49613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Agent dependencies defined!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class FalkorDependencies:\n",
    "    \"\"\"Dependencies for the Falkor agent.\"\"\"\n",
    "    chat_session: ChatSession\n",
    "\n",
    "print(\"üì¶ Agent dependencies defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb562e0",
   "metadata": {},
   "source": [
    "## 7. Create the Pydantic AI Agent\n",
    "\n",
    "Define our main agent with a comprehensive system prompt for UFC knowledge graph querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e090fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Falkor Agent created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the Falkor agent\n",
    "falkor_agent = Agent(\n",
    "    get_model(),\n",
    "    system_prompt=\"\"\"You are a knowledge graph assistant that helps users query a FalkorDB knowledge graph.\n",
    "\n",
    "When a user provides ANY input (questions, entity names, keywords, or statements), you MUST use the search_falkor tool with the EXACT, COMPLETE user input as the query parameter. Do not modify, shorten, or extract keywords from the user's input.\n",
    "\n",
    "The knowledge graph system is designed to handle:\n",
    "- Full questions: \"Who is Salsa Boy?\"\n",
    "- Entity names: \"Salsa Boy\"\n",
    "- Keywords: \"fighters\", \"matches\", \"UFC\"\n",
    "- Statements: \"Show me information about recent fights\"\n",
    "- Any other text input\n",
    "\n",
    "The tool will return:\n",
    "- A Cypher query that was generated to search the graph (automatically adapted to the input type)\n",
    "- Context data extracted from the graph using that query\n",
    "\n",
    "After receiving the results, explain what the Cypher query does and interpret the context data to provide a helpful answer. Focus on the entities, relationships, and graph patterns found in the results. If the input was just an entity name, provide comprehensive information about that entity and its connections.\n",
    "Try to be concise but thorough in your explanations, ensuring the user understands the graph data and its implications.\"\"\",\n",
    "    deps_type=FalkorDependencies\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Falkor Agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececa87d",
   "metadata": {},
   "source": [
    "## 8. Define the Search Result Model\n",
    "\n",
    "Create a Pydantic model to structure the results from our FalkorDB search tool.\n",
    "\n",
    "This model ensures type safety and clear data structure for the agent's search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2125ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Search result model defined!\n"
     ]
    }
   ],
   "source": [
    "class FalkorSearchResult(BaseModel):\n",
    "    \"\"\"Model representing a search result from FalkorDB.\"\"\"\n",
    "    cypher: str = Field(description=\"The generated Cypher query\")\n",
    "    context: str = Field(description=\"The extracted context from the knowledge graph\")\n",
    "\n",
    "print(\"üìä Search result model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62de651",
   "metadata": {},
   "source": [
    "## 9. Register the Search Tool\n",
    "\n",
    "Create and register the main tool that allows our agent to search the FalkorDB knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f08b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Search tool registered with agent!\n"
     ]
    }
   ],
   "source": [
    "@falkor_agent.tool\n",
    "async def search_falkor(ctx: RunContext[FalkorDependencies], query: str) -> List[FalkorSearchResult]:\n",
    "    \"\"\"Search the FalkorDB knowledge graph with the given query - returns only cypher and context.\n",
    "    \n",
    "    Args:\n",
    "        ctx: The run context containing dependencies\n",
    "        query: The search query to find information in the knowledge graph\n",
    "        \n",
    "    Returns:\n",
    "        A list of search results containing cypher queries and context that match the query\n",
    "    \"\"\"\n",
    "    # Access the ChatSession from dependencies\n",
    "    chat_session = ctx.deps.chat_session\n",
    "    \n",
    "    try:\n",
    "        # Use the chat session's generate_cypher_query method to get cypher and context\n",
    "        # This method returns (context, cypher) tuple\n",
    "        context, cypher = chat_session.generate_cypher_query(query)\n",
    "        \n",
    "        # Format the result to match the expected interface\n",
    "        formatted_result = FalkorSearchResult(\n",
    "            cypher=cypher or '',\n",
    "            context=context or ''\n",
    "        )\n",
    "        \n",
    "        return [formatted_result]\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        print(f\"Error searching FalkorDB: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"üîç Search tool registered with agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24961420",
   "metadata": {},
   "source": [
    "## 10. Gradio Chat Function\n",
    "\n",
    "Create the Gradio integration function for web-based chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727dda29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Gradio chat function defined!\n"
     ]
    }
   ],
   "source": [
    "async def gradio_chat_function(message, history):\n",
    "    \"\"\"\n",
    "    Gradio chat function that processes user messages and returns agent responses.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user's message\n",
    "        history (List): Chat history from Gradio\n",
    "        \n",
    "    Returns:\n",
    "        str: The agent's response\n",
    "    \"\"\"\n",
    "    deps = FalkorDependencies(chat_session=chat_session)\n",
    "    \n",
    "    try:\n",
    "        # Convert Gradio history to our message format\n",
    "        message_history = []\n",
    "        for user_msg, assistant_msg in history:\n",
    "            if user_msg:\n",
    "                message_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if assistant_msg:\n",
    "                message_history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        \n",
    "        # Get response from agent\n",
    "        result = await falkor_agent.run(\n",
    "            message,\n",
    "            deps=deps\n",
    "        )\n",
    "        \n",
    "        return result.data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "print(\"üåê Gradio chat function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99435d7",
   "metadata": {},
   "source": [
    "## 11. Create Gradio Web Interface\n",
    "\n",
    "Build an elegant web-based chat interface using Gradio for easy interaction with the UFC knowledge graph agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4873577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Gradio interface created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210415/1172376482.py:23: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=400, show_copy_button=True)\n"
     ]
    }
   ],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"Create a clean, elegant Gradio chat interface.\"\"\"\n",
    "    \n",
    "    def chat(message, history):\n",
    "        \"\"\"Handle chat interactions.\"\"\"\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        try:\n",
    "            import asyncio\n",
    "            response = asyncio.run(gradio_chat_function(message, history))\n",
    "            history.append((message, response))\n",
    "            return history, \"\"\n",
    "        except Exception as e:\n",
    "            history.append((message, f\"‚ùå Error: {str(e)}\"))\n",
    "            return history, \"\"\n",
    "    \n",
    "    # Create interface with minimal, clean design\n",
    "    with gr.Blocks(title=\"ü•ã UFC Knowledge Graph Agent\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"# ü•ã UFC Knowledge Graph Agent\\n### Ask anything about UFC fighters, fights, and events!\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(height=400, show_copy_button=True)\n",
    "        \n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(placeholder=\"Ask me about UFC...\", scale=4, container=False)\n",
    "            submit = gr.Button(\"üöÄ Ask\", variant=\"primary\")\n",
    "            clear = gr.Button(\"üóëÔ∏è\", variant=\"secondary\")\n",
    "        \n",
    "        # Example questions\n",
    "        gr.Examples([\n",
    "            \"Who is Salsa Boy?\",\n",
    "            \"Show me recent UFC fights\",\n",
    "            \"Which fighters have the most wins?\",\n",
    "            \"What are the UFC weight classes?\"\n",
    "        ], msg)\n",
    "        \n",
    "        # Event handling\n",
    "        submit.click(chat, [msg, chatbot], [chatbot, msg])\n",
    "        msg.submit(chat, [msg, chatbot], [chatbot, msg])\n",
    "        clear.click(lambda: [], outputs=chatbot)\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and launch interface\n",
    "gradio_interface = create_gradio_interface()\n",
    "print(\"üåê Gradio interface created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e293",
   "metadata": {},
   "source": [
    "## 12. Launch the Interface\n",
    "\n",
    "Launch the Gradio web interface and start chatting with the UFC knowledge graph agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8586ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching UFC Knowledge Graph Agent...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://79db0b9f73212d5729.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "* Running on public URL: https://79db0b9f73212d5729.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://79db0b9f73212d5729.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210415/2419845010.py:29: DeprecationWarning: `result.data` is deprecated, use `result.output` instead.\n",
      "  return result.data\n"
     ]
    }
   ],
   "source": [
    "# Launch the interface\n",
    "print(\"üöÄ Launching UFC Knowledge Graph Agent...\")\n",
    "gradio_interface.launch(share=True, inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
