{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage of the GraphRAG-SDK to Create a Knowledge Graph and RAG System\n",
    "\n",
    "The following example demonstrates the basic usage of this SDK to create a GraphRAG using URLs with auto-detected ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphrag_sdk[litellm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubeli/miniconda3/envs/sdk/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from graphrag_sdk.source import URL\n",
    "from graphrag_sdk import KnowledgeGraph, Ontology\n",
    "from graphrag_sdk.models.litellm import LiteModel\n",
    "from graphrag_sdk.model_config import KnowledgeGraphModelConfig\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "logging.disable(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI credential in .env file\n",
    "# OPENAI_API_KEY=your_api_key\n",
    "\n",
    "# Get your Credentials from Falkor website, see https://app.falkordb.cloud\n",
    "falkor_host = \"\"\n",
    "falkor_port = None\n",
    "falkor_username = \"\"\n",
    "falkor_password = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Source Data from URLs\n",
    "\n",
    "This example uses rottentomatoes URL files as the source data. We will import these files as `Source` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.rottentomatoes.com/m/side_by_side_2012\",\n",
    "\"https://www.rottentomatoes.com/m/matrix\",\n",
    "\"https://www.rottentomatoes.com/m/matrix_revolutions\",\n",
    "\"https://www.rottentomatoes.com/m/matrix_reloaded\",\n",
    "\"https://www.rottentomatoes.com/m/speed_1994\",\n",
    "\"https://www.rottentomatoes.com/m/john_wick_chapter_4\"]\n",
    "\n",
    "sources = [URL(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ontology from the Sources\n",
    "\n",
    "Next, we will utilize an LLM to automatically extract ontology from the data. We will also add `boundaries` to the ontology detection process to ensure the desired ontology is accurately identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Documents: 100%|██████████| 7/7 [00:43<00:00,  6.16s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LiteModel(model_name=\"openai/gpt-4.1\")\n",
    "\n",
    "boundaries = \"\"\"\n",
    "    Extract only the most relevant information about all the movies, actors, and directors over the text.\n",
    "    Avoid creating entities for details that can be expressed as attributes.\n",
    "\"\"\"\n",
    "\n",
    "ontology = Ontology.from_sources(\n",
    "    sources=sources,\n",
    "    boundaries=boundaries,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KG from Sources and Ontology\n",
    "\n",
    "Create a Knowledge Graph (KG) from the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Documents: 100%|██████████| 6/6 [00:59<00:00,  9.86s/it]\n"
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph(\n",
    "    name=\"movies\",\n",
    "    model_config=KnowledgeGraphModelConfig.with_model(model),\n",
    "    ontology=ontology,\n",
    "    host=falkor_host,\n",
    "    port=falkor_port,\n",
    "    username=falkor_username,\n",
    "    password=falkor_password\n",
    ")\n",
    "kg.process_sources(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph RAG\n",
    "\n",
    "Utilize the `chat_session` method to start a conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is the director of the movie The Matrix? \n",
      "A: Lilly Wachowski and Lana Wachowski are the directors of the movie The Matrix.\n",
      "\n",
      "\n",
      "Q: How this director connected to Keanu Reeves? \n",
      "A: Lilly Wachowski and Lana Wachowski directed movies in which Keanu Reeves acted.\n",
      "\n",
      "\n",
      "Q: Who is the director of the movie Side by Side? \n",
      "A: Christopher Kenneally is the director of the movie Side by Side.\n",
      "\n",
      "\n",
      "Q: Order the directors that you mentioned in all of our conversation by lexical order. \n",
      "A: The directors in lexical order are: Christopher Kenneally, Lana Wachowski, and Lilly Wachowski.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = kg.chat_session()\n",
    "\n",
    "answer = chat.send_message(\"Who is the director of the movie The Matrix?\")\n",
    "print(f\"Q: {answer['question']} \\nA: {answer['response']}\\n\")\n",
    "\n",
    "answer = chat.send_message(\"How this director connected to Keanu Reeves?\")\n",
    "print(f\"Q: {answer['question']} \\nA: {answer['response']}\\n\")\n",
    "\n",
    "answer = chat.send_message(\"Who is the director of the movie Side by Side?\")\n",
    "print(f\"Q: {answer['question']} \\nA: {answer['response']}\\n\")\n",
    "\n",
    "answer = chat.send_message(\"Order the directors that you mentioned in all of our conversation by lexical order.\")\n",
    "print(f\"Q: {answer['question']} \\nA: {answer['response']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
